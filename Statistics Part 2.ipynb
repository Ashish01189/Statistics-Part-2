{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3d14993d-efd4-4404-be63-319d61b86e1a",
      "cell_type": "markdown",
      "source": "# Statistics Part 2",
      "metadata": {}
    },
    {
      "id": "a6b0f60d-b674-452c-bf21-c50f24e09f3d",
      "cell_type": "code",
      "source": "# 1. What is hypothesis testing in statistics?\n    # Hypothesis testing in statistics is a method used to make decisions or inferences about a population based on a sample of data. It helps determine whether there is enough evidence in a sample to support a specific claim or assumption (called a hypothesis) about the population.\n\n# Key Concepts:\n    # 1. Null Hypothesis (H₀):\n        # The default or initial assumption.\n        # Usually states that there is no effect or no difference.\n        # Example: \"The average height of adult males is 170 cm.\"\n\n    # 2. Alternative Hypothesis (H₁ or Ha):\n        # The claim you want to test.\n        # It represents a new effect or difference.\n        # Example: \"The average height of adult males is not 170 cm.\"\n\n    # 3. Test Statistic:\n        # A value calculated from sample data that is used to test the hypothesis.\n        # Example: Z-score or t-score.\n\n    # 4. P-value:\n        # The probability of observing the test results under the assumption that the null hypothesis is true.\n        # A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis.\n\n    # 5. Significance Level (α):\n        # The threshold set by the researcher to decide whether to reject H₀.\n        # Common values: 0.05, 0.01.\n\n    # 6. Decision:\n        # If p-value ≤ α → Reject the null hypothesis.\n        # If p-value > α → Do not reject the null hypothesis.\n\n    # Example:\n    # Suppose a factory claims that their light bulbs last at least 1000 hours on average.\n        # H₀: μ = 1000 (bulbs last 1000 hours)\n        # H₁: μ < 1000 (bulbs last less than 1000 hours)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0ecad20-8d2d-410b-b4d1-8078eb032e31",
      "cell_type": "code",
      "source": "# 2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n    # The null hypothesis and the alternative hypothesis are two opposing statements used in hypothesis testing. They help statisticians decide whether there is enough evidence in a sample to support a specific claim about a population.\n\n    # Null Hypothesis (H₀):\n        # The null hypothesis is a statement that there is no effect, no difference, or nothing unusual happening.\n        # It serves as the starting point or default assumption in hypothesis testing.\n\n        # Example:\n        # \"The average weight of apples in a batch is equal to 150 grams.\"\n        # H₀: μ = 150\n\n    # Alternative Hypothesis (H₁ or Ha):\n        # The alternative hypothesis is a statement that contradicts the null hypothesis. It proposes that there is an effect, a difference, or a change.\n        # It's what you aim to support with your data.\n\n        # Example:\n        # \"The average weight of apples in a batch is not equal to 150 grams.\"\n        # H₁: μ ≠ 150\n\n    # Main Difference:\n        # The null hypothesis is what you assume to be true unless the data strongly suggests otherwise. The alternative hypothesis is what you're trying to support with the data.\n\n        # In hypothesis testing, you never prove the null or alternative hypothesis absolutely. Instead, you collect data, run a test, and then decide whether the data gives you enough reason to reject the null hypothesis in favor of the alternative.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e92b8a1a-d98c-4675-b6cc-afb9674ffc62",
      "cell_type": "code",
      "source": "# 3. What is the significance level in hypothesis testing, and why is it important?\n    # The significance level in hypothesis testing, often denoted by α (alpha), is the threshold used to decide whether the result of a test is statistically significant.\n\n    # What is the significance level (α)?\n        # It represents the probability of rejecting the null hypothesis when it is actually true — in other words, it's the chance of making a Type I error.\n        # Common values for α are:\n            # 0.05 (5%) — most commonly used\n            # 0.01 (1%)\n            # 0.10 (10%)\n        # For example, if α = 0.05, you are willing to accept a 5% chance of wrongly rejecting a true null hypothesis.\n   \n    # Why is it important?\n        # 1. Sets the bar for evidence:\n            # The significance level determines how strong the evidence must be to reject the null hypothesis. A lower α means stronger evidence is needed.\n\n        # 2. Controls error rate:\n            # It limits the risk of a false positive — saying there is an effect or difference when there actually isn't one.\n\n        # 3. Helps decision-making:\n            # In real-world applications like medical research or quality control, choosing an appropriate α is crucial. For instance, in drug testing, a low α like 0.01 might be chosen to minimize the risk of approving an ineffective drug.\n\n    # How it’s used in a test:\n        # 1. You calculate the p-value from your test.\n        # 2. Then you compare the p-value to α:\n            # If p-value ≤ α, reject the null hypothesis (result is significant).\n            # If p-value > α, do not reject the null (not enough evidence).\n\n    # Example:\n    # Suppose you're testing whether a new teaching method improves student scores. You set α = 0.05. After analyzing the data, you get a p-value of 0.03.\n    # Since 0.03 < 0.05, you reject the null hypothesis and conclude the teaching method likely has an effect.\n    # In short, the significance level helps you control the risk of making incorrect conclusions based on sample data. It's a fundamental part of hypothesis testing because it defines how cautious or confident you want to be with your decision.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f2eaddcd-d1fe-4aa2-8069-3c7e272f2606",
      "cell_type": "code",
      "source": "# 4. What does a P-value represent in hypothesis testing?\n    # The p-value in hypothesis testing represents the probability of obtaining results as extreme as (or more extreme than) the observed results, assuming that the null hypothesis is true.\n\n# In Simple Terms:\n    # A p-value tells you how surprising your data is under the assumption that the null hypothesis (H₀) is correct.\n        # A small p-value means the data is unlikely under H₀ — suggesting evidence against H₀.\n        # A large p-value means the data is consistent with H₀ — so you do not reject H₀.\n\n# How to Interpret It:\n    # p ≤ 0.05 - There is statistically significant evidence against the null hypothesis.\n    # You reject the null hypothesis.\n\n    # p > 0.05 → The evidence is not strong enough to reject the null hypothesis.\n    # You do not reject the null.\n\n# Example:\n    # Suppose a company claims their light bulbs last at least 1000 hours. You test a sample and compute a p-value of 0.02.\n        # This means: If the bulbs really last 1000 hours on average (null hypothesis is true), there is only a 2% chance of getting results like yours.\n        # Since 0.02 < 0.05, you reject the null hypothesis and suspect the bulbs might last less than 1000 hours.\n\n# Important Notes:\n    # The p-value is NOT the probability that the null hypothesis is true.\n    # It also is not the probability that the alternative hypothesis is true.\n    # It only tells you how compatible your data is with the null hypothesis.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d69faaf4-9295-4fcd-80c1-0913674d94f7",
      "cell_type": "code",
      "source": "# 5. How do you interpret the P-value in hypothesis testing?\n    # The P-value is the probability of obtaining results at least as extreme as the ones observed, assuming that the null hypothesis (H₀) is true.\n\n    # Interpretation:\n        # Small P-value (≤ α, e.g., 0.05):\n            # There is strong evidence against the null hypothesis.\n            # You reject H₀.\n            # The result is statistically significant.\n\n        # Large P-value (> α):\n            # There is weak evidence against the null hypothesis.\n            # You fail to reject H₀.\n            # The result is not statistically significant.\n\n    # Example:\n    # Suppose you're testing a new drug and get a P-value = 0.03 with α = 0.05:\n        # Since 0.03 < 0.05, you reject the null hypothesis.\n        # Interpretation: There's a 3% chance that the observed effect (or more extreme) would occur if the drug actually had no effect.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f6232ae0-823b-4a4a-b62c-c11e2292eb94",
      "cell_type": "code",
      "source": "# 6. What are Type 1 and Type 2 errors in hypotesis testing?\n    # In hypothesis testing, Type I and Type II errors are two kinds of mistakes you can make when making a decision based on sample data.\n\n# 1. Type I Error (False Positive)\n    # Definition: Rejecting the null hypothesis (H₀) when it is actually true.\n    # Example: You conclude a new medicine works, but in reality, it doesn't.\n    # Probability of Type I Error: Denoted by α (alpha), which is the significance level (commonly 0.05).\n    # Think of it as: A false alarm — detecting an effect that isn’t there.\n\n# 2. Type II Error (False Negative)\n    # Definition: Failing to reject the null hypothesis (H₀) when it is actually false.\n    # Example: You conclude a medicine doesn’t work, but in reality, it does.\n    # Probability of Type II Error: Denoted by β (beta).\n    # Think of it as: Missing a real effect — failing to detect something that actually exists.\n\n# Trade-off:\n    # Lowering the chance of a Type I error (by lowering α) increases the risk of a Type II error, and vice versa.\n    # That's why power analysis (1 - β) is used to balance both errors when designing experiments.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed5447ca-f5a3-4edf-bc7c-b1227be25840",
      "cell_type": "code",
      "source": "# 7. What is the difference beetween a one-tailed and a two-tailed test in hypothesis testing?\n    # The main difference between a one-tailed and a two-tailed test in hypothesis testing lies in what kind of difference you are testing for — whether it's in one direction or both directions.\n\n    # 1. One-Tailed Test (Directional Test):\n        # Use when: You are testing for an effect in one specific direction (either greater than or less than).\n        # Hypotheses Example (right-tailed):\n            # H₀: μ ≤ 100\n            # H₁: μ > 100\n        # Area of rejection: Only one end (tail) of the distribution.\n        # Purpose: To detect whether a parameter is significantly greater than or less than a certain value, but not both.\n\n    # 2. Two-Tailed Test (Non-directional Test):\n        # Use when: You are testing for any significant difference, in either direction.\n        # Hypotheses Example:\n            # H₀: μ = 100\n            # H₁: μ ≠ 100\n        # Area of rejection: Both ends (tails) of the distribution.\n        # Purpose: To detect whether a parameter is different from a certain value, either higher or lower.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e6c2cb59-5dd9-40ac-bfd7-0dc61eb8fcd8",
      "cell_type": "code",
      "source": "# 8. What is the Z-test, and when is it used in hypothesis testing?\n    # The Z-test is a type of hypothesis test used to determine whether there is a significant difference between sample and population means (or between two sample means), when the population standard deviation is known.\n\n    # When to Use a Z-test:\n        # You use a Z-test when all of the following conditions are met:\n            # 1. The sample size is large (n ≥ 30), or the population is normally distributed.\n            # 2. The population standard deviation (σ) is known.\n            # 3. You are testing a mean (or sometimes a proportion).\n\n    # Common Uses of Z-test:\n        # 1. One-sample Z-test:\n            # To compare a sample mean to a known population mean.\n            # Example: Is the average height of students in a college different from the national average?\n\n        # 2. Two-sample Z-test:\n            # To compare means of two independent samples (with known variances).\n            # Example: Are average test scores different between two schools?\n\n        # 3. Z-test for proportions:\n            # To compare observed proportion to expected proportion (or between two proportions).\n            # Example: Is the proportion of voters supporting a candidate significantly different from 50%?\n\n    # What the Z-score Tells You:\n        # The Z-value shows how many standard deviations your sample mean is from the population mean.\n        # You compare the Z-value to critical Z-values (like ±1.96 for a 95% confidence level in a two-tailed test).\n        # Or you use the Z-score to calculate a P-value.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c24fdd91-6e8f-42d9-a177-9efa7064ceaf",
      "cell_type": "code",
      "source": "# 9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n    # A Z-score (or standard score) tells you how many standard deviations a data point or sample statistic is from the population mean.\n    # In hypothesis testing, it helps you determine whether to reject the null hypothesis.\n\n#  What the Z-score Represents:\n    # It tells you how far your sample mean is from the population mean, in standard deviation units.\n    # A high absolute Z-score means the sample mean is far from the population mean, which may indicate a significant difference.\n# Example:\n    # Let’s say:\n        # Population mean (μ) = 100\n        # Population standard deviation (σ) = 10\n        # Sample mean (𝑋ˉ) = 105\n        # Sample size (n) = 36\n\n    # Then,\n            # Z= (105−100)/(10/*root36)\n            #  = 5/(10/6)\n            #  = 5/1.6667 ≈3\n        # Interpretation: A Z-score of 3 means the sample mean is 3 standard deviations above the population mean → very unlikely under H₀ → statistically significant.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8642722d-a0d8-4f45-ad17-3227bf31f2fb",
      "cell_type": "code",
      "source": "# 10. What is the T-distribution, and when should it be used instead  of the normal distribution?\n    # The T-distribution (also called Student’s t-distribution) is a probability distribution used in hypothesis testing and confidence intervals when dealing with small sample sizes and unknown population standard deviation.\n    # It looks similar to the normal distribution, but has heavier tails, meaning it accounts for more variability — especially with smaller samples.\n\n# When to Use the T-distribution Instead of the Normal Distribution:\n    # Use the T-distribution when:\n        # 1. Sample size is small (typically n < 30).\n        # 2. Population standard deviation (σ) is unknown.\n        # 3. Data is approximately normally distributed (especially important for small samples).\n\n# How is the T-distribution different?\n    # The key difference is that the T-distribution accounts for more uncertainty when estimating the population standard deviation using a small sample. Because of this, it spreads out more (especially in the tails). This means you're less likely to incorrectly conclude that a result is significant due to sampling variability. As the sample size increases, the T-distribution starts to look more like the normal distribution, because the estimates become more accurate.\n\n# Example Use Case:\n    # Suppose you want to test if a new teaching method improves student scores:\n        # 1. You only have 15 students (n = 15).\n        # 2. You don’t know the population standard deviation.\n    # We should use a T-test with T-distribution instead of a Z-test.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9fa0594d-fafc-45f3-858a-d8837844c648",
      "cell_type": "code",
      "source": "# 11. What is the difference between a Z-test and a T-test?\n    # The main difference between a Z-test and a T-test lies in the sample size and whether the population standard deviation (σ) is known.\n    # Here’s a clear explanation without using a table:\n\n    # Z-test:\n        # 1. Use a Z-test when the population standard deviation is known.\n        # 2. It’s generally used when you have a large sample size, usually 30 or more.\n        # 3. The Z-test relies on the normal distribution, which assumes less uncertainty because the population parameters are known.\n        # For example, if you’re testing whether the average height of a large group of people is 170 cm and you know the standard deviation of the entire population, you would use a Z-test.\n\n    # T-test:\n        # 1. Use a T-test when the population standard deviation is unknown.\n        # 2. It’s best suited for small samples, usually less than 30.\n        # 3. The T-test uses the t-distribution, which accounts for more variability (i.e., wider tails) to handle the extra uncertainty from estimating the standard deviation from the sample itself.\n        # For instance, if you’re testing the average test score of a class of 15 students and don’t know the population standard deviation, you’d use a T-test.\n\n    # Summary:\n        # Z-test: Known σ, large sample → more precise, uses normal curve.\n        # T-test: Unknown σ, small sample → more cautious, uses wider t-curve.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4dee1327-d2e0-43b1-99d4-f0ecfb7159f6",
      "cell_type": "code",
      "source": "# 12. What is the T-test, and how is it used in hypothesis testing?\n    # The T-test is a statistical method used in hypothesis testing to determine whether there is a significant difference between:\n        # 1. A sample mean and a population mean,\n        # 2. Two sample means,\n        # 3. Or paired observations (like before-and-after measurements).\n    # It is used when the population standard deviation is unknown, and the sample size is small (typically less than 30). The T-test relies on the T-distribution, which adjusts for extra variability due to estimating the standard deviation from the sample.\n\n    # When to Use the T-test:\n        # Use the T-test when:\n            # 1. You're comparing means (not proportions or variances).\n            # 2. The population standard deviation is unknown.\n            # 3. The data is approximately normally distributed.\n            # 4. The sample size is small.\n\n    # Types of T-tests:\n        # 1. One-sample T-test\n            # Tests whether the sample mean is significantly different from a known population mean.\n            # Example: Is the average score of your class significantly different from the national average?\n\n        # 2. Independent two-sample T-test\n            # Compares the means of two independent groups.\n            # Example: Do boys and girls score differently on a math test?\n\n        # 3. Paired sample T-test (dependent)\n            # Compares means from the same group at two different times (before and after).\n            # Example: Did students improve their test scores after a training program?\n\n    # Purpose in Hypothesis Testing:\n        # The T-test helps decide whether the difference you observe is likely due to random chance or represents a real effect. Based on the result:\n            # If the P-value ≤ α (e.g., 0.05), you reject the null hypothesis.\n            # If the P-value > α, you fail to reject the null hypothesis.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "054baed8-628c-494c-bf28-b7a29ae9287b",
      "cell_type": "code",
      "source": "# 13. What is the relationship between Z-test and T-test in hypothesis testing?\n    # The Z-test and T-test are closely related in hypothesis testing — they both serve the same purpose: to determine whether the difference between means (or a mean and a known value) is statistically significant. The main difference lies in the conditions under which each test is used.\n\n    # Relationship Between Z-test and T-test:\n        # 1. Both are parametric tests\n            # They assume the data is normally distributed, especially important when sample sizes are small. They compare sample data to a population parameter or another sample to draw conclusions.\n\n        # 2. Both use similar test structures\n            # The formulas are very similar:\n\n            # Z-test:\n                        # Z= (Xˉ−μ)/(σ/root n)\n \n            # T-test:\n                        # t= (Xˉ−μ)/(s/)n\n \n        # The key difference is that:\n            # The Z-test uses the population standard deviation (σ).\n            # The T-test uses the sample standard deviation (s) because σ is unknown.\n\n        # 3. T-test generalizes the Z-test\n            # The T-test is more flexible because it can be used when σ is unknown. When the sample size is large, the T-distribution becomes very close to the normal distribution, and the T-test results approach those of the Z-test.\n\n        # 4. Which to use depends on data conditions\n            # If you know the population standard deviation and have a large sample, use the Z-test.\n            # If you don’t know the population standard deviation or have a small sample, use the T-test.\n\n    # Summary in Words:\n        # The Z-test and T-test both test means, but the T-test is used when there's more uncertainty (i.e., smaller sample size, unknown standard deviation). As the sample size increases, the T-distribution behaves like the normal distribution, and both tests give almost the same results.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a5ea8c19-661e-443c-a12d-6290d7502828",
      "cell_type": "code",
      "source": "# 14. What is a confidence interval, and how is it used to interpret statistical result?\n    # A confidence interval (CI) is a range of values, derived from sample data, that is likely to contain the true population parameter (such as the mean or proportion) with a certain level of confidence.\n    # It gives you an estimate of uncertainty around a sample statistic.\n\n# Example:\n    # If you calculate a 95% confidence interval for the average height of students and get:\n                    #  CI=(168.2 cm, 171.8 cm)\n    # This means you are 95% confident that the true average height of all students lies between 168.2 cm and 171.8 cm.\n    # It does not mean there’s a 95% probability the mean is in that range — rather, if we repeated the study many times, 95% of the confidence intervals we compute would contain the true mean.\n\n# Formula (for mean with known or large sample):\n                    # CI= Xˉ±Z ∗(σ/root n)\n    # Where:\n        # 𝑋ˉ= sample mean\n        # σ = population standard deviation (or use sample standard deviation for T-distribution)\n        # n = sample size\n        # Z∗ = critical value from Z-distribution (e.g., 1.96 for 95% confidence)\n    # If σ is unknown and the sample is small, use the T-distribution instead.\n\n# How is it used to interpret statistical results?\n    # 1. Estimate population values: Instead of giving just a single number (point estimate), the CI provides a range, offering a better picture of uncertainty.\n    # 2. Decision making in hypothesis testing:\n            # If the confidence interval does not include the null hypothesis value (e.g., a mean of 0 or a difference of 0), you reject the null hypothesis.\n            # If it does include the null value, the result is not statistically significant at that confidence level.\n    # 3. Precision indicator:\n            # A narrower interval means more precision, and a wider interval means more uncertainty. Larger samples give tighter (more precise) intervals.\n\n# Summary:\n    # A confidence interval helps you understand the reliability of your estimate. Instead of saying, “The average weight is 70 kg,” a CI allows you to say, “We are 95% confident that the average weight lies between 68 and 72 kg.”",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bc77dcd7-91c3-4bcc-a4c6-40e60f614be2",
      "cell_type": "code",
      "source": "# 15. What is the margin of error, and how does it affect the confidence interval?\n    # The margin of error (MoE) is a number that shows how much the results of a sample survey or experiment might differ from the true population value. It reflects the degree of uncertainty in your sample estimate.\n    # In simple terms, it's how much the sample result might go up or down when trying to estimate the real value in the population.\n\n# Example:\n    # If you survey people and find that 60% support a policy with a margin of error of ±4%, it means the true support in the population is likely between 56% and 64%.\n\n# How does it affect the confidence interval?\n    # The confidence interval (CI) is calculated using the margin of error:\n        # Confidence Interval = Sample Estimate ± Margin of Error\n    # So, the margin of error defines how wide or narrow the interval is:\n    # A larger margin of error → wider interval → less precision.\n    # A smaller margin of error → narrower interval → more precision.\n\n# What influences the margin of error?\n    # 1. Sample size:\n        # Larger samples give smaller margins of error because the estimate becomes more stable.\n\n    # 2. Confidence level:\n        # A higher confidence level (like 99% instead of 95%) makes the margin of error larger, because you’re casting a wider “net” to be more sure.\n\n    # Data variability:\n        # If your data varies a lot (high standard deviation), the margin of error increases.\n\n    # Margin of Error Formula (for sample mean):\n                # MoE=Z∗× (σ/root n)\n        # Where:\n            # 𝑍∗ = Z-score for your confidence level (e.g., 1.96 for 95%)\n            # σ = population standard deviation (or sample standard deviation s)\n            # n = sample size\n\n    # In short:\n        # The margin of error measures how uncertain your estimate is. It determines the width of your confidence interval. A smaller margin means a tighter range and more confidence in your estimate's precision.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3a53877b-b0eb-473e-bdf9-4da1e007b6e0",
      "cell_type": "code",
      "source": "# 16. How is Boyes' Theorem used in statistics, and what is its significance?\n    # Bayes' Theorem is a fundamental rule in probability and statistics used to update the probability of a hypothesis based on new evidence or data.\n    # It helps answer questions like:\n        # “Given that something has occurred, how likely is it that a certain condition was true beforehand?”\n\n# Formula for Bayes’ Theorem:\n            # P(A∣B)= (P(B∣A)⋅P(A))/P(B)\n\n        # Where:\n            # P(A∣B): Posterior probability – the probability of event A given B has occurred (what we want to find)\n            # P(B∣A): Likelihood – the probability of observing B given A is true\n            # P(A): Prior probability – the initial belief about A before seeing B\n            # P(B): Evidence – the overall probability of B occurring\n\n# How it's used in statistics:\n    # Bayes' Theorem is used to:\n        # 1. Update probabilities when new data is available.\n        # 2. Revise beliefs about a hypothesis as evidence accumulates.\n        # 3. Build Bayesian models for decision-making and forecasting.\n\n# Real-world example:\n    # Let’s say a disease affects 1% of people (P(Disease)=0.01). A test is 99% accurate for both detecting disease when present and not giving false positives. You test positive — how likely is it that you actually have the disease?\n    # Bayes’ Theorem lets us calculate:\n            # P(Disease∣Positive Test)\n    # Despite the test being accurate, the actual probability you have the disease may be surprisingly low, because false positives can occur in a large number of healthy people. This is counterintuitive, but very common in medical testing.\n\n# Significance:\n    # More accurate reasoning under uncertainty: Helps in fields like medicine, finance, and AI.\n    # Core of Bayesian statistics: Unlike classical statistics, which focuses on fixed parameters, Bayesian methods allow probabilities to change as new data comes in.\n    # Risk assessment: Useful in spam detection, diagnostics, quality control, and more.\n\n# In summary:\n    # Bayes' Theorem is a powerful way to update beliefs using data. It shifts probability from what you initially believe (prior) to what you should believe after seeing the evidence (posterior). It’s especially valuable when dealing with uncertain or incomplete information.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a122c116-ab45-4bd9-ac62-001fd9af0220",
      "cell_type": "code",
      "source": "# 17. What is the Chi-square distribution, and when is it used?\n    # The Chi-square (χ²) distribution is a probability distribution that is commonly used in inferential statistics, especially for testing relationships between categorical variables and evaluating goodness of fit.\n    # It is skewed to the right, and its shape depends on the degrees of freedom (df). As df increases, the distribution becomes more symmetric and bell-shaped.\n\n    # Key Characteristics:\n        # 1. It is only defined for non-negative values (χ² ≥ 0).\n        # 2. It is used when dealing with variances and frequencies, not means.\n        # 3. It arises from the sum of the squares of standard normal variables.\n\n    # When is the Chi-square distribution used?\n        # Here are the main scenarios where it's used:\n\n    # 1. Chi-square test for independence\n        # Used to test whether two categorical variables are independent or related.\n        # Example:\n            # Does gender affect choice of product category?\n            # (You compare observed frequencies vs expected frequencies in a contingency table.)\n\n    # 2. Chi-square goodness-of-fit test\n        # Used to test whether a sample distribution fits a theoretical distribution.\n        # Example:\n            # Do the outcomes of a dice roll match a uniform distribution (i.e., each number from 1–6 is equally likely)?\n\n    # 3. Chi-square test for homogeneity\n        # Used to compare the distribution of a categorical variable across different populations.\n        # Example:\n            # Do different age groups have the same preference for a brand?\n\n    # Significance in statistics:\n        # 1. Helps determine if differences in data are due to chance or reflect a real pattern.\n        # 2. Commonly used in survey analysis, market research, medical studies, and quality control.\n        # 3. Forms the basis of tests for model fit in logistic regression and other statistical models.\n\n    # In summary:\n        # The Chi-square distribution is a tool for working with categorical data. It’s primarily used to test how well observed data match expected results, or whether two variables are related. If the computed Chi-square statistic is large enough, it suggests a significant difference or association.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5cd5f22b-889b-4dc7-8369-6cf8ef8da0b2",
      "cell_type": "code",
      "source": "# 18. What is the Chi-square goodness of fit test, and how is it applied?\n    # The Chi-square goodness of fit test is a statistical method used to determine how well an observed frequency distribution matches an expected distribution. It helps assess whether sample data fits a theoretical model or hypothesis.\n\n    # Purpose:\n        # To test if the observed data matches what we would expect to see based on a specific hypothesis (e.g., equal distribution, known proportions).\n\n    # Real-life example:\n        # Suppose a die is rolled 60 times, and the results are:\n            # 1: 5 times\n            # 2: 10 times\n            # 3: 8 times\n            # 4: 12 times\n            # 5: 15 times\n            # 6: 10 times\n        # If the die is fair, we would expect each number to appear 10 times (60 rolls ÷ 6 sides). The goodness of fit test checks if the differences between observed and expected counts are due to chance or indicate a biased die.\n\n    # Steps to Apply the Chi-square Goodness of Fit Test:\n        # 1. State the hypotheses:\n            # H0 : The data follows the expected distribution.\n            # H1 : The data does not follow the expected distribution.\n\n        # 2. Calculate expected frequencies based on the theoretical distribution.\n        # 3. Compute the Chi-square statistic using the formula above.\n        # 4. Find degrees of freedom (df):\n              # df=number of categories−1\n        # 5. Compare the Chi-square value to the critical value from the Chi-square table (based on df and significance level, e.g., 0.05).\n        # 6. Make a decision:\n            # If the Chi-square statistic is greater than the critical value, reject 𝐻0\n            # Otherwise, fail to reject 𝐻0.\n\n    # In summary:\n        # The Chi-square goodness of fit test checks whether your observed data distribution fits the expected pattern. It is widely used in quality control, genetics, survey research, and any situation where expected proportions are known or assumed.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "05045811-1026-4ebc-8653-c50c62f94a0a",
      "cell_type": "code",
      "source": "# 19. What is the F-distribution, and what are its assumptions?\n    # The F-distribution is a continuous probability distribution that arises frequently in ANOVA (Analysis of Variance) and tests that compare variances. It's used to determine whether the variances of two populations are significantly different.\n    # It is right-skewed, and its exact shape depends on two degrees of freedom:\n        # One for the numerator (df₁)\n        # One for the denominator (df₂)\n\n    # When is the F-distribution used?\n        # 1. ANOVA tests (to compare means across multiple groups by analyzing variance)\n        # 2. F-test for equality of variances (to compare two population variances)\n        # 3. In regression analysis (to test overall model significance)\n\n    # Example:\n        # In a one-way ANOVA, you use the F-distribution to test:\n        # “Are the means of three or more groups significantly different?”\n        # You calculate an F-statistic by dividing the variance between groups by the variance within groups. If the ratio is large, it suggests real differences between groups.\n\n    # F-statistic formula (for ANOVA):\n                # 𝐹 = Mean Square Between Groups (MSB)/ Mean Square Within Groups (MSW)\n     # If the null hypothesis (all group means are equal) is true, the F-statistic should be close to 1.\n\n    # Assumptions of the F-distribution:\n        # For valid results, the F-test or ANOVA that uses the F-distribution must meet these assumptions:\n\n    # 1. Independence:\n        # The observations in each group must be independent of each other.\n\n    # 2. Normality:\n        # Each group’s data should be approximately normally distributed, especially important when sample sizes are small.\n\n    # 3. Equal variances (homogeneity of variance):\n        # All groups being compared should have roughly equal variances (especially in ANOVA).\n    \n    # In summary:\n        # The F-distribution is used to compare variances and evaluate if group differences are significant.\n        # It is commonly applied in ANOVA and regression analysis.\n        # Valid results depend on assumptions like normality, equal variances, and independence.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "42e35c7d-0192-491d-b0ba-32fbccb3e75c",
      "cell_type": "code",
      "source": "# 20. What is an ANOVA test, and what are its assumptions?\n    # ANOVA stands for Analysis of Variance. It is a statistical method used to determine whether there are any significant differences between the means of three or more independent groups.\n    # Instead of comparing means two at a time (like multiple t-tests), ANOVA tests all groups simultaneously, reducing the risk of error.\n\n    # Why use ANOVA?\n        # “Are the differences among group means greater than we'd expect by chance?”\n\n    # Basic idea:\n        # ANOVA compares two types of variance:\n            # 1. Between-group variance (how much the group means differ from the overall mean)\n            # 2. Within-group variance (how much individual data points differ from their own group mean)\n            # If the between-group variance is much larger than the within-group variance, the group means are likely not all the same.\n\n    # Types of ANOVA:\n        # 1. One-way ANOVA – Tests the effect of one factor (e.g., comparing test scores of students in 3 different schools).\n        # 2. Two-way ANOVA – Tests the effect of two factors (e.g., school and teaching method).\n        # 3. Repeated measures ANOVA – When the same subjects are tested under different conditions.\n\n    # Hypotheses in One-Way ANOVA:\n        # Null hypothesis (𝐻0): All group means are equal\n                    # 𝜇1=𝜇2=𝜇3=…\n        # Alternative hypothesis (𝐻1): At least one group mean is different\n\n    # F-statistic (test statistic):\n            # 𝐹 = Variance between groups / Variance within groups\n         # A higher F-value suggests a greater chance that the group means are not all equal.\n\n    # Assumptions of ANOVA:\n        # To trust the results of an ANOVA, these assumptions must be met:\n        # 1. Independence: Each observation should be independent from the others.\n        # 2. Normality: The data in each group should be approximately normally distributed, especially important if sample sizes are small.\n        # 3. Homogeneity of variances: All groups should have roughly equal variances (this is tested using Levene’s Test or Bartlett’s Test).\n\n    # summary:\n        # ANOVA is used to test whether three or more group means differ significantly.\n        # It uses the F-distribution to assess variance patterns.\n        # Assumptions include normality, equal variances, and independence.\n        # If ANOVA shows a significant result, follow-up tests (like Tukey’s HSD) can identify which groups differ.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a6b4adad-8589-4a58-a61b-c67d48022530",
      "cell_type": "code",
      "source": "# 21. What are the different types of ANOVA test?\n    # There are several types of ANOVA (Analysis of Variance) tests, each designed for different kinds of experimental designs or data structures. Here's a clear breakdown of the main types of ANOVA:\n\n    # 1. One-Way ANOVA\n            # Purpose: Compares the means of three or more independent groups based on one independent variable (factor).\n            # Example: Comparing test scores of students from three different schools.\n        # Use when:\n            # You have one categorical independent variable (e.g., school)\n            # You want to see if it affects one numerical dependent variable (e.g., test score)\n\n    # 2. Two-Way ANOVA\n            # Purpose: Examines the effect of two independent variables on a dependent variable, and also their interaction.\n            # Example: Studying the effect of teaching method and gender on test scores.\n        # Use when:\n            # You have two factors, and you want to study:\n            # Their individual effects\n            # Any interaction between them\n\n    # 3. Repeated Measures ANOVA\n            # Purpose: Compares means when the same subjects are measured multiple times (like in before/after studies or across time points).\n            # Example: Measuring weight loss of participants every week for 3 months.\n        # Use when:\n            # You measure the same individuals under different conditions or times.\n            # It accounts for within-subject variability.\n\n    # 4. Two-Way Repeated Measures ANOVA\n            # Purpose: Similar to repeated measures ANOVA, but includes two within-subject factors, or one within and one between.\n            # Example: Measuring test scores across different time points (within-subject) and between different classrooms (between-subject).\n\n    # 5. Mixed-Design ANOVA (Split-Plot ANOVA)\n            # Purpose: Combines both between-subjects and within-subjects variables.\n            # Example: Testing performance of two groups (between-subjects: treatment vs control) over multiple trials (within-subjects: time).\n\n    # Use when:\n        # Your study design includes both repeated measures and independent group comparisons.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4803d26b-a0c3-429b-9207-3ebf8897b2a1",
      "cell_type": "code",
      "source": "# 22. What is the F-test, and how does it relate to hypothesis testing?\n    # The F-test is a statistical test used to compare two variances or to test whether multiple means are equal (as in ANOVA). It helps determine whether the observed differences between groups are statistically significant or likely due to random chance.\n    # It is named after Ronald Fisher, who developed the method.\n\n    # What does the F-test do?\n        # The F-test is based on the F-distribution, which is the ratio of two variances. It’s used to test hypotheses like:\n            # Are two variances significantly different?\n            # Are the group means significantly different (in ANOVA)?\n            # Does a regression model explain a significant amount of variability in the data?\n\n    # F-test statistic formula:\n            # 𝐹 = Variance1/Variance2\n        # Or in ANOVA:\n            # F= Mean Square Within Groups/Mean Square Between Groups\n        # A large F-value suggests more variation between groups than within them, which may indicate a significant difference.\n\n    # How it relates to hypothesis testing:\n        # The F-test follows the standard hypothesis testing procedure:\n        # 1. State the hypotheses:\n            # Null hypothesis (𝐻0): The variances (or group means) are equal\n            # Alternative hypothesis (𝐻1): At least one variance (or group mean) differs\n        # 2. Choose a significance level (e.g., α = 0.05)\n        # 3. Calculate the F-statistic\n        # 4. Compare it with the critical F-value from the F-distribution table (based on degrees of freedom)\n        # 5. Make a decision:\n                # If F>Fcritical: reject the null hypothesis\n                # If F≤Fcritical: do not reject the null hypothesis\n\n    # Common applications of the F-test:\n        # ANOVA: To test if three or more means are significantly different.\n        # Equality of variances test: Comparing variance between two samples.\n        # Regression analysis: Testing if the overall model is significant (i.e., if at least one predictor explains variation in the dependent variable).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0e663a1f-0e24-4180-bcd2-e402220f5f1a",
      "cell_type": "markdown",
      "source": "# Practical",
      "metadata": {}
    },
    {
      "id": "8005e288-6681-4efa-a107-b56602ced7ef",
      "cell_type": "code",
      "source": "# 1.  Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and inerpret the results.\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef one_sample_z_test(sample_data, population_mean, population_std, alpha=0.05):\n   \n    sample_mean = np.mean(sample_data)\n    sample_size = len(sample_data)\n\n    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    print(f\"Sample Mean = {sample_mean:.3f}\")\n    print(f\"Z-Score = {z_score:.3f}\")\n    print(f\"P-Value = {p_value:.4f}\")\n\n    if p_value < alpha:\n        print(f\"Result: Reject the null hypothesis (p < {alpha})\")\n        print(\"Interpretation: There is a statistically significant difference between the sample mean and the population mean.\")\n    else:\n        print(f\"Result: Fail to reject the null hypothesis (p ≥ {alpha})\")\n        print(\"Interpretation: There is no statistically significant difference between the sample mean and the population mean.\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65a88dae-6fe4-411b-a41b-2cc8a31f52a3",
      "cell_type": "code",
      "source": "# 2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n\nimport numpy as np\nfrom scipy.stats import norm\n\nnp.random.seed(42)\n\npopulation_mean = 170 \npopulation_std = 10 \nsample_size = 50  \n\nsample_data = np.random.normal(loc=172, scale=10, size=sample_size)\n\nsample_mean = np.mean(sample_data)\n\nz_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n\np_value = 2 * (1 - norm.cdf(abs(z_score)))\n\nprint(\"Simulated Sample Mean:\", round(sample_mean, 2))\nprint(\"Z-Score:\", round(z_score, 3))\nprint(\"P-Value:\", round(p_value, 4))\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"Result: Reject the null hypothesis.\")\n    print(\"Interpretation: Sample mean is significantly different from the population mean.\")\nelse:\n    print(\"Result: Fail to reject the null hypothesis.\")\n    print(\"Interpretation: No significant difference between sample and population means.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dd7e61c4-9700-46b0-996b-6022577ebdde",
      "cell_type": "code",
      "source": "# 3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean.\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef one_sample_z_test(sample_data, population_mean, population_std, alpha=0.05):\n\n    sample_mean = np.mean(sample_data)\n    sample_size = len(sample_data)\n\n    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    print(f\"Sample Mean: {sample_mean:.2f}\")\n    print(f\"Z-Score: {z_score:.3f}\")\n    print(f\"P-Value: {p_value:.4f}\")\n\n    if p_value < alpha:\n        print(f\"Result: Reject the null hypothesis (p < {alpha})\")\n        print(\"Interpretation: Sample mean is significantly different from the population mean.\")\n    else:\n        print(f\"Result: Fail to reject the null hypothesis (p ≥ {alpha})\")\n        print(\"Interpretation: No significant difference between sample and population means.\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a1b39de-b956-4146-9664-b346af74d42b",
      "cell_type": "code",
      "source": "# 4. Perform a two-tailed z-test using Python and visualize the decision region on a plot.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef z_test_with_visualization(sample_data, population_mean, population_std, alpha=0.05):\n    sample_mean = np.mean(sample_data)\n    sample_size = len(sample_data)\n\n    z_score = (sample_mean - population_mean) / (population_std / np.sqrt(sample_size))\n\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    z_critical = norm.ppf(1 - alpha / 2)\n\n    print(f\"Sample Mean: {sample_mean:.2f}\")\n    print(f\"Z-Score: {z_score:.3f}\")\n    print(f\"P-Value: {p_value:.4f}\")\n    print(f\"Critical Z-Scores: ±{z_critical:.3f}\")\n\n    if abs(z_score) > z_critical:\n        print(\"Result: Reject the null hypothesis.\")\n    else:\n        print(\"Result: Fail to reject the null hypothesis.\")\n\n    x = np.linspace(-4, 4, 1000)\n    y = norm.pdf(x)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, label='Standard Normal Distribution')\n\n    plt.fill_between(x, y, where=(x <= -z_critical), color='red', alpha=0.4, label='Rejection Region')\n    plt.fill_between(x, y, where=(x >= z_critical), color='red', alpha=0.4)\n\n    plt.axvline(z_score, color='blue', linestyle='--', label=f'Z = {z_score:.2f}')\n\n    plt.axvline(-z_critical, color='black', linestyle='--', label=f'-Zₐ/₂ = {-z_critical:.2f}')\n    plt.axvline(z_critical, color='black', linestyle='--', label=f'Zₐ/₂ = {z_critical:.2f}')\n\n    plt.title('Two-Tailed Z-Test Decision Regions')\n    plt.xlabel('Z-Score')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nnp.random.seed(42)\nsample_data = np.random.normal(loc=102, scale=15, size=50)  # Simulated sample\n\npopulation_mean = 100\npopulation_std = 15\n\nz_test_with_visualization(sample_data, population_mean, population_std, alpha=0.05)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9190010-3bc3-4a26-9370-56234239d4e0",
      "cell_type": "code",
      "source": "# 5. Create a Python function that calcuates and visualizes Type1 and Type2 error during hypothesis testing.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef visualize_type1_type2_errors(mu0, mu1, sigma, n, alpha=0.05):\n    \"\"\"\n    Visualizes Type I and Type II errors for a two-tailed Z-test.\n    \n    Parameters:\n        mu0 : float : Population mean under H0\n        mu1 : float : True population mean under H1 (for power analysis)\n        sigma : float : Known population standard deviation\n        n : int : Sample size\n        alpha : float : Significance level (default 0.05)\n    \"\"\"\n\n    se = sigma / np.sqrt(n)\n \n    z_critical = norm.ppf(1 - alpha / 2)\n\n    lower_crit = mu0 - z_critical * se\n    upper_crit = mu0 + z_critical * se\n\n    beta = norm.cdf(upper_crit, loc=mu1, scale=se) - norm.cdf(lower_crit, loc=mu1, scale=se)\n    power = 1 - beta\n\n    x = np.linspace(mu0 - 4*se, mu1 + 4*se, 1000)\n\n    y_h0 = norm.pdf(x, loc=mu0, scale=se)\n    y_h1 = norm.pdf(x, loc=mu1, scale=se)\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(x, y_h0, label='H₀ Distribution (μ = {:.1f})'.format(mu0), color='blue')\n    plt.plot(x, y_h1, label='H₁ Distribution (μ = {:.1f})'.format(mu1), color='green')\n\n    plt.fill_between(x, y_h0, where=(x < lower_crit) | (x > upper_crit), color='red', alpha=0.3, label='Type I Error (α)')\n\n    plt.fill_between(x, y_h1, where=(x >= lower_crit) & (x <= upper_crit), color='orange', alpha=0.3, label='Type II Error (β)')\n\n    plt.axvline(lower_crit, color='black', linestyle='--')\n    plt.axvline(upper_crit, color='black', linestyle='--')\n\n    plt.title(\"Type I and Type II Errors in Hypothesis Testing\")\n    plt.xlabel(\"Sample Mean\")\n    plt.ylabel(\"Probability Density\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    print(f\"Critical Values: [{lower_crit:.2f}, {upper_crit:.2f}]\")\n    print(f\"Type II Error (β): {beta:.4f}\")\n    print(f\"Power of the test (1 - β): {power:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a2e7b18-441e-4dcc-b4a9-b286fed11c28",
      "cell_type": "code",
      "source": "# 6. Write a python program to perform an independent T-test and interpret the result.\n\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\ndef independent_t_test(sample1, sample2, alpha=0.05, equal_var=True):\n    \"\"\"\n    Performs an independent two-sample t-test and interprets the results.\n    \n    Parameters:\n    sample1 : array-like : Data from group 1\n    sample2 : array-like : Data from group 2\n    alpha : float : Significance level (default 0.05)\n    equal_var : bool : Assume equal variances? (True = standard T-test, False = Welch's T-test)\n    \"\"\"\n    t_stat, p_value = ttest_ind(sample1, sample2, equal_var=equal_var)\n\n    print(f\"Sample 1 Mean: {np.mean(sample1):.2f}\")\n    print(f\"Sample 2 Mean: {np.mean(sample2):.2f}\")\n    print(f\"T-Statistic: {t_stat:.4f}\")\n    print(f\"P-Value: {p_value:.4f}\")\n    \n    if p_value < alpha:\n        print(f\"Result: Reject the null hypothesis (p < {alpha})\")\n        print(\"Interpretation: There is a significant difference between the two sample means.\")\n    else:\n        print(f\"Result: Fail to reject the null hypothesis (p ≥ {alpha})\")\n        print(\"Interpretation: No significant difference between the two sample means.\")\n\nnp.random.seed(42)\ngroup1 = np.random.normal(loc=100, scale=10, size=30)  # Group 1 with mean = 100\ngroup2 = np.random.normal(loc=105, scale=10, size=30)  # Group 2 with mean = 105\n\nindependent_t_test(group1, group2, alpha=0.05, equal_var=True)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2b376485-34f2-4f1a-8bbd-0a7457d8f2e5",
      "cell_type": "code",
      "source": "# 7. Perform a paired sample T-test using Python and visualize the comparison results.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_rel\n\ndef paired_t_test_with_visualization(before, after, alpha=0.05):\n    \"\"\"\n    Performs a paired t-test and visualizes the comparison.\n\n    Parameters:\n        before : array-like : First set of measurements (e.g., before treatment)\n        after : array-like : Second set of measurements (e.g., after treatment)\n        alpha : float : Significance level (default = 0.05)\n    \"\"\"\n\n    t_stat, p_value = ttest_rel(before, after)\n\n    print(f\"Mean Before: {np.mean(before):.2f}\")\n    print(f\"Mean After: {np.mean(after):.2f}\")\n    print(f\"T-Statistic: {t_stat:.4f}\")\n    print(f\"P-Value: {p_value:.4f}\")\n\n    if p_value < alpha:\n        print(f\"Result: Reject the null hypothesis (p < {alpha})\")\n        print(\"Interpretation: Significant difference between paired observations.\")\n    else:\n        print(f\"Result: Fail to reject the null hypothesis (p ≥ {alpha})\")\n        print(\"Interpretation: No significant difference between paired observations.\")\n\n    x = np.arange(len(before)) + 1\n    plt.figure(figsize=(10, 5))\n    plt.plot(x, before, marker='o', label='Before')\n    plt.plot(x, after, marker='s', label='After')\n    for i in range(len(before)):\n        plt.plot([x[i], x[i]], [before[i], after[i]], color='gray', linestyle='--', alpha=0.5)\n    \n    plt.title(\"Paired Sample Comparison\")\n    plt.xlabel(\"Sample Index\")\n    plt.ylabel(\"Measurement Value\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\nnp.random.seed(42)\nbefore_treatment = np.random.normal(loc=100, scale=10, size=20)\nafter_treatment = before_treatment + np.random.normal(loc=-2, scale=5, size=20)\n\npaired_t_test_with_visualization(before_treatment, after_treatment)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97126531-6232-47dc-a0f6-8a441b3ae31d",
      "cell_type": "code",
      "source": "# 8. Simulate data and perform both Z-test and T-test, then compare the result using python.\n\nimport numpy as np\nfrom scipy.stats import norm, t\n\ndef simulate_data_and_compare_tests(mu_population=100, sigma_population=15, sample_size=30, true_mean=102, alpha=0.05):\n\n    np.random.seed(42)\n    sample = np.random.normal(loc=true_mean, scale=sigma_population, size=sample_size)\n    sample_mean = np.mean(sample)\n    sample_std = np.std(sample, ddof=1)\n\n    se_z = sigma_population / np.sqrt(sample_size)\n    z_score = (sample_mean - mu_population) / se_z\n    p_value_z = 2 * (1 - norm.cdf(abs(z_score)))\n\n    se_t = sample_std / np.sqrt(sample_size)\n    t_score = (sample_mean - mu_population) / se_t\n    p_value_t = 2 * (1 - t.cdf(abs(t_score), df=sample_size - 1))\n\n    print(\"\\nSample Summary\")\n    print(f\"Sample Mean: {sample_mean:.2f}\")\n    print(f\"Sample Std Dev: {sample_std:.2f}\")\n    print(f\"Sample Size: {sample_size}\")\n\n    print(\"\\nZ-Test (Population Std Known)\")\n    print(f\"Z-Score: {z_score:.4f}\")\n    print(f\"P-Value (Z): {p_value_z:.4f}\")\n    print(\"=>\", \"Reject H0\" if p_value_z < alpha else \"Fail to Reject H0\")\n\n    print(\"\\nT-Test (Population Std Unknown)\")\n    print(f\"T-Score: {t_score:.4f}\")\n    print(f\"P-Value (T): {p_value_t:.4f}\")\n    print(\"=>\", \"Reject H0\" if p_value_t < alpha else \"Fail to Reject H0\")\n\n    print(\"\\nComparison:\")\n    if (p_value_z < alpha) == (p_value_t < alpha):\n        print(\"✔ Both tests agree on the hypothesis decision.\")\n    else:\n        print(\"✖ The tests give different results. Consider sample size or assumption validity.\")\n\nsimulate_data_and_compare_tests()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "022c873c-68a9-467e-9f0d-0b0f5fcef3ad",
      "cell_type": "code",
      "source": "# 9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n\nimport numpy as np\nfrom scipy.stats import t\n\ndef confidence_interval_mean(sample, confidence=0.95):\n    \"\"\"\n    Calculate the confidence interval for the sample mean using t-distribution.\n    \n    Parameters:\n        sample : array-like : The sample data\n        confidence : float : Confidence level (default 0.95 for 95%)\n    \n    Returns:\n        (lower_bound, upper_bound) : Tuple with the confidence interval\n    \"\"\"\n    sample = np.array(sample)\n    n = len(sample)\n    mean = np.mean(sample)\n    std_err = np.std(sample, ddof=1) / np.sqrt(n)\n    df = n - 1\n    t_crit = t.ppf((1 + confidence) / 2, df)\n\n    margin_of_error = t_crit * std_err\n    lower_bound = mean - margin_of_error\n    upper_bound = mean + margin_of_error\n\n    return (lower_bound, upper_bound)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "51f681b8-1a28-4415-8468-042fa67efc50",
      "cell_type": "code",
      "source": "# 10. Write a Python program to calculate the margin of error for a given confidence level using sample data.\n\nimport numpy as np\nfrom scipy.stats import t\n\ndef margin_of_error(sample, confidence=0.95):\n    \"\"\"\n    Calculate the margin of error for the sample mean using t-distribution.\n\n    Parameters:\n        sample : array-like : Sample data\n        confidence : float : Confidence level (default 0.95)\n\n    Returns:\n        margin_of_error : float : The margin of error for the sample mean\n    \"\"\"\n    sample = np.array(sample)\n    n = len(sample)\n    std_dev = np.std(sample, ddof=1)\n    se = std_dev / np.sqrt(n)\n\n    df = n - 1\n    t_crit = t.ppf((1 + confidence) / 2, df)\n\n    moe = t_crit * se\n    return moe\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "679dd506-a5aa-41d8-aaae-7361985e7f21",
      "cell_type": "code",
      "source": "# 11. Implement a Bayesian inference methon using Baye's Theorem in Pyton and explain the process.\n\ndef bayes_theorem(prior, sensitivity, specificity):\n    \"\"\"\n    Calculate the posterior probability using Bayes' Theorem.\n\n    Parameters:\n        prior : float : P(Disease) = prior probability of having the disease\n        sensitivity : float : P(Positive | Disease) = true positive rate\n        specificity : float : P(Negative | No Disease) = true negative rate\n\n    Returns:\n        posterior : float : P(Disease | Positive)\n    \"\"\"\n\n    p_positive = (sensitivity * prior) + ((1 - specificity) * (1 - prior))\n    \n    posterior = (sensitivity * prior) / p_positive\n    return posterior\n\nprior = 0.01\nsensitivity = 0.99\nspecificity = 0.95 \n\nposterior = bayes_theorem(prior, sensitivity, specificity)\nprint(f\"Probability of disease given a positive test: {posterior:.4f} ({posterior*100:.2f}%)\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dceb3f28-c749-499e-afc8-c9d168d13adf",
      "cell_type": "code",
      "source": "# 12. Perfom a Chi-square test for independence between two categorical variables in Python.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndata = np.array([[30, 20],\n                 [45, 35]])\n\ndf = pd.DataFrame(data, \n                  index=[\"Male\", \"Female\"], \n                  columns=[\"Purchase\", \"No Purchase\"])\n\nprint(\"Contingency Table:\")\nprint(df)\n\nchi2, p, dof, expected = chi2_contingency(df)\n\nprint(f\"\\nChi-square Statistic: {chi2:.4f}\")\nprint(f\"Degrees of Freedom: {dof}\")\nprint(f\"P-value: {p:.4f}\")\nprint(\"\\nExpected Frequencies:\")\nprint(pd.DataFrame(expected, index=[\"Male\", \"Female\"], columns=[\"Purchase\", \"No Purchase\"]))\n\nalpha = 0.05\nif p < alpha:\n    print(\"\\nConclusion: Reject the null hypothesis — variables are dependent.\")\nelse:\n    print(\"\\nConclusion: Fail to reject the null hypothesis — no evidence of dependence.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7508f3da-fbf0-481e-9f2f-0726ea0e525e",
      "cell_type": "code",
      "source": "# 13. Write a Python program to calculate the expected frequencies for a Chi-square test on observed data.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n\nobserved = np.array([\n    [30, 20],\n    [45, 35]\n])\n\ndf_observed = pd.DataFrame(observed, \n                           index=[\"Male\", \"Female\"], \n                           columns=[\"Purchase\", \"No Purchase\"])\n\nprint(\"Observed Frequencies:\")\nprint(df_observed)\n\nchi2, p, dof, expected = chi2_contingency(observed)\n\ndf_expected = pd.DataFrame(expected, \n                           index=[\"Male\", \"Female\"], \n                           columns=[\"Purchase\", \"No Purchase\"])\n\nprint(\"\\nExpected Frequencies (if independent):\")\nprint(df_expected.round(2))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bfe56117-4710-4c4c-87da-0c918b69cac0",
      "cell_type": "code",
      "source": "# 14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution.\n\nimport numpy as np\nfrom scipy.stats import chisquare\n\nobserved = np.array([8, 12, 9, 11, 10, 10])\n\nexpected = np.array([10, 10, 10, 10, 10, 10])\n\nchi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n\nprint(\"Chi-square Statistic:\", round(chi2_stat, 4))\nprint(\"P-value:\", round(p_value, 4))\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: observed data does NOT fit expected distribution.\")\nelse:\n    print(\"Fail to reject the null hypothesis: observed data fits the expected distribution.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33123174-27d6-4d3b-9be9-40b60eb1ff6f",
      "cell_type": "code",
      "source": "# 15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2\n\nplt.figure(figsize=(10, 6))\n\ndfs = [1, 2, 5, 10, 20]\nx = np.linspace(0, 40, 500)\n\nfor df in dfs:\n    y = chi2.pdf(x, df)\n    plt.plot(x, y, label=f'df = {df}')\n\nplt.title(\"Chi-square Distribution for Various Degrees of Freedom\", fontsize=14)\nplt.xlabel(\"Chi-square value\")\nplt.ylabel(\"Probability Density\")\nplt.legend(title=\"Degrees of Freedom\")\nplt.grid(True)\nplt.tight_layout()\n\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3758784f-529d-499c-8941-a71053e9a2f7",
      "cell_type": "code",
      "source": "# 16. Implement an F-test using Python to compare the variance of two random samples.\n\nimport numpy as np\nfrom scipy.stats import f\n\nnp.random.seed(0)\nsample1 = np.random.normal(loc=10, scale=5, size=30)\nsample2 = np.random.normal(loc=10, scale=3, size=25)\n\nvar1 = np.var(sample1, ddof=1)\nvar2 = np.var(sample2, ddof=1)\n\nif var1 > var2:\n    F = var1 / var2\n    dfn = len(sample1) - 1\n    dfd = len(sample2) - 1\nelse:\n    F = var2 / var1\n    dfn = len(sample2) - 1\n    dfd = len(sample1) - 1\n\np_value = 2 * min(f.cdf(F, dfn, dfd), 1 - f.cdf(F, dfn, dfd))\n\nprint(\"Variance of Sample 1:\", var1)\nprint(\"Variance of Sample 2:\", var2)\nprint(\"F-statistic:\", F)\nprint(\"Degrees of Freedom: \", dfn, dfd)\nprint(\"P-value:\", p_value)\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: Variances are significantly different.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference in variances.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eda4bb5f-0263-4484-a3ce-f53417a0bd45",
      "cell_type": "code",
      "source": "# 17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results.\n\nimport numpy as np\nfrom scipy import stats\n\ngroup1 = [88, 92, 85, 91, 87]\ngroup2 = [78, 74, 80, 73, 77]\ngroup3 = [84, 86, 83, 89, 90]\n\nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n\nprint(\"ANOVA F-statistic:\", f_statistic)\nprint(\"P-value:\", p_value)\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: At least one group mean is significantly different.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between group means.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0d3431c9-70b8-4bf3-9d46-cc9abec35453",
      "cell_type": "code",
      "source": "# 18. Perform a one-way ANOVA tests using Python to compare the mean of different groups and plot the results.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport pandas as pd\n\ngroup1 = [88, 92, 85, 91, 87]\ngroup2 = [78, 74, 80, 73, 77]\ngroup3 = [84, 86, 83, 89, 90]\n\nf_stat, p_value = stats.f_oneway(group1, group2, group3)\n\ndata = pd.DataFrame({\n    'Score': group1 + group2 + group3,\n    'Group': ['Method A'] * len(group1) + ['Method B'] * len(group2) + ['Method C'] * len(group3)\n})\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='Group', y='Score', data=data, palette='Set2')\nplt.title('Score Distribution by Teaching Method (ANOVA)')\nplt.ylabel('Score')\nplt.grid(True)\nplt.show()\n\nprint(\"ANOVA F-statistic:\", round(f_stat, 3))\nprint(\"P-value:\", round(p_value, 4))\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: At least one group mean is significantly different.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference between group means.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f073a4af-df7b-4192-896d-46de64541466",
      "cell_type": "code",
      "source": "# 19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.stats.diagnostic import lilliefors\nfrom statsmodels.stats.api import levene\nfrom statsmodels.stats.anova import anova_lm\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef check_anova_assumptions(data, group_col, value_col):\n    print(\"\\nNormality Check (Shapiro-Wilk):\")\n    for group in data[group_col].unique():\n        scores = data[data[group_col] == group][value_col]\n        stat, p = stats.shapiro(scores)\n        print(f\"  {group}: W = {stat:.3f}, p = {p:.3f}\")\n        if p > 0.05:\n            print(\"   → Data looks normal.\")\n        else:\n            print(\"   → Data may not be normally distributed.\")\n    \n    print(\"\\nHomogeneity of Variance (Levene’s Test):\")\n    groups = [group[value_col].values for name, group in data.groupby(group_col)]\n    stat, p = stats.levene(*groups)\n    print(f\"  Levene’s statistic = {stat:.3f}, p = {p:.3f}\")\n    if p > 0.05:\n        print(\"   → Variances are equal.\")\n    else:\n        print(\"   → Variances may not be equal.\")\n\n    print(\"\\nIndependence Check (Residual Plot):\")\n    model = ols(f'{value_col} ~ C({group_col})', data=data).fit()\n    residuals = model.resid\n    plt.figure(figsize=(8, 4))\n    plt.scatter(model.fittedvalues, residuals)\n    plt.axhline(y=0, color='red', linestyle='--')\n    plt.title(\"Residual Plot\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residuals\")\n    plt.grid(True)\n    plt.show()\n    print(\"   → Look for random scatter around 0 (no pattern).\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "709e2da0-5805-4e3b-af37-f1e96fe1225e",
      "cell_type": "code",
      "source": "# 20. Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nnp.random.seed(1)\ndata = pd.DataFrame({\n    'Score': np.random.normal(75, 10, 60),\n    'Method': ['A'] * 20 + ['B'] * 20 + ['C'] * 20,\n    'Gender': ['Male'] * 10 + ['Female'] * 10 + ['Male'] * 10 + ['Female'] * 10 + ['Male'] * 10 + ['Female'] * 10\n})\n\ndata.loc[(data['Method'] == 'A') & (data['Gender'] == 'Female'), 'Score'] += 5\ndata.loc[(data['Method'] == 'C') & (data['Gender'] == 'Male'), 'Score'] -= 4\n\nmodel = ols('Score ~ C(Method) + C(Gender) + C(Method):C(Gender)', data=data).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\n\nprint(\"\\nTwo-Way ANOVA Table:\\n\")\nprint(anova_table)\n\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=data, x='Method', y='Score', hue='Gender', capsize=0.1, palette='Set1', dodge=True)\nplt.title('Interaction Plot: Method vs Gender on Score')\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "70ffe1a8-f93a-45fd-87c5-2745b72bffae",
      "cell_type": "code",
      "source": "# 21. Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import f\n\ndfn = 5 \ndfd = 20 \n\nx = np.linspace(0, 5, 1000)\ny = f.pdf(x, dfn, dfd)\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label=f'F-distribution (dfn={dfn}, dfd={dfd})', color='blue')\nplt.fill_between(x[x > f.ppf(0.95, dfn, dfd)], 0, y[x > f.ppf(0.95, dfn, dfd)],\n                 color='red', alpha=0.3, label='Rejection Region (α = 0.05)')\n\nplt.title(\"F-Distribution\")\nplt.xlabel(\"F value\")\nplt.ylabel(\"Probability Density\")\nplt.legend()\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bfead3a0-41c6-4391-8b7b-5399193aec23",
      "cell_type": "code",
      "source": "# 22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nnp.random.seed(42)\ngroup_a = np.random.normal(loc=70, scale=5, size=30)\ngroup_b = np.random.normal(loc=75, scale=5, size=30)\ngroup_c = np.random.normal(loc=80, scale=5, size=30)\n\ndf = pd.DataFrame({\n    'Score': np.concatenate([group_a, group_b, group_c]),\n    'Group': ['A'] * len(group_a) + ['B'] * len(group_b) + ['C'] * len(group_c)\n})\n\nf_stat, p_value = stats.f_oneway(group_a, group_b, group_c)\n\nprint(\"\\nOne-Way ANOVA Results:\")\nprint(f\"F-statistic = {f_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: At least one group mean is significantly different.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference between group means.\")\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='Group', y='Score', data=df, palette='Set2')\nplt.title(\"Boxplot of Scores by Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Score\")\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0f907206-db29-4db1-bb31-a54cfe255f68",
      "cell_type": "code",
      "source": "# 23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means.\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(42)\ngroup1 = np.random.normal(loc=50, scale=5, size=100)\ngroup2 = np.random.normal(loc=53, scale=5, size=100) \n\nt_stat, p_value = stats.ttest_ind(group1, group2)\n\nprint(\"Two-Sample T-Test Results:\")\nprint(f\"T-statistic = {t_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: The means are significantly different.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference between means.\")\n\nsns.histplot(group1, kde=True, color='blue', label='Group 1', stat='density')\nsns.histplot(group2, kde=True, color='orange', label='Group 2', stat='density')\nplt.axvline(np.mean(group1), color='blue', linestyle='--')\nplt.axvline(np.mean(group2), color='orange', linestyle='--')\nplt.title('Distribution of Simulated Data')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "318d1f48-af22-40d9-b8c4-9b4e525616db",
      "cell_type": "code",
      "source": "# 24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the result.\n\nimport numpy as np\nfrom scipy.stats import chi2\n\nnp.random.seed(42)\ndata = np.random.normal(loc=50, scale=5, size=30)\nn = len(data)\nsample_variance = np.var(data, ddof=1) \n\nsigma0_squared = 25\n\nchi2_stat = (n - 1) * sample_variance / sigma0_squared\n\nalpha = 0.05\ndf = n - 1\np_value = 2 * min(chi2.cdf(chi2_stat, df), 1 - chi2.cdf(chi2_stat, df))\n\nprint(f\"Sample variance = {sample_variance:.4f}\")\nprint(f\"Chi-square statistic = {chi2_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: The population variance is significantly different from the hypothesized value.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference in variance.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0264b66-4838-42cc-9a8e-644b25b4bf77",
      "cell_type": "code",
      "source": "# 25. Write a Python script to perform a Z-test for comparing proportions between two datasets or groups.\n\nimport numpy as np\nfrom statsmodels.stats.proportion import proportions_ztest\n\nsuccesses = np.array([40, 55])  \nn_obs = np.array([100, 120])   \n\nz_stat, p_value = proportions_ztest(count=successes, nobs=n_obs, alternative='two-sided')\n\nprint(\"Z-Test for Proportions\")\nprint(f\"Z-statistic = {z_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: Proportions are significantly different.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference in proportions.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "45eec2bc-c857-48a4-9ddc-219e5ecd0d1d",
      "cell_type": "code",
      "source": "# 26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import f\n\nnp.random.seed(0)\ndata1 = np.random.normal(loc=50, scale=10, size=30)\ndata2 = np.random.normal(loc=55, scale=5, size=30) \n\nvar1 = np.var(data1, ddof=1)\nvar2 = np.var(data2, ddof=1)\nn1, n2 = len(data1), len(data2)\ndf1, df2 = n1 - 1, n2 - 1\n\nF_stat = var1 / var2\np_value = 2 * min(f.cdf(F_stat, df1, df2), 1 - f.cdf(F_stat, df1, df2))\n\nprint(\"F-Test for Equality of Variances\")\nprint(f\"Variance of Data1 = {var1:.2f}\")\nprint(f\"Variance of Data2 = {var2:.2f}\")\nprint(f\"F-statistic = {F_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: Variances are significantly different.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: No significant difference in variances.\")\n\nx = np.linspace(0.1, 5, 500)\ny = f.pdf(x, df1, df2)\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label=\"F-distribution\", color='blue')\nplt.axvline(F_stat, color='red', linestyle='--', label=f\"F-statistic = {F_stat:.2f}\")\nplt.fill_between(x[x > f.ppf(0.975, df1, df2)], 0, y[x > f.ppf(0.975, df1, df2)],\n                 color='red', alpha=0.3, label=\"Rejection Region (α=0.05)\")\nplt.fill_between(x[x < f.ppf(0.025, df1, df2)], 0, y[x < f.ppf(0.025, df1, df2)],\n                 color='red', alpha=0.3)\nplt.title(\"F-Distribution and Test Statistic\")\nplt.xlabel(\"F Value\")\nplt.ylabel(\"Probability Density\")\nplt.legend()\nplt.grid(True)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "25f81326-c852-4979-983d-9f0971f38eb9",
      "cell_type": "code",
      "source": "# 27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n\nimport numpy as np\nfrom scipy.stats import chisquare\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nobserved = np.random.multinomial(120, [1/6]*6)\n\nexpected = [120 / 6] * 6 \n\nchi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n\nprint(\"Chi-Square Goodness-of-Fit Test\")\nprint(f\"Observed frequencies: {observed}\")\nprint(f\"Expected frequencies: {expected}\")\nprint(f\"Chi-square statistic = {chi2_stat:.4f}\")\nprint(f\"P-value = {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"→ Reject the null hypothesis: Observed data does not fit the expected distribution.\")\nelse:\n    print(\"→ Fail to reject the null hypothesis: Observed data fits the expected distribution.\")\n\ncategories = [f\"Side {i+1}\" for i in range(6)]\nx = np.arange(len(categories))\nwidth = 0.35\n\nplt.bar(x - width/2, observed, width=width, label='Observed', color='skyblue')\nplt.bar(x + width/2, expected, width=width, label='Expected', color='orange')\nplt.xticks(x, categories)\nplt.ylabel(\"Frequency\")\nplt.title(\"Observed vs Expected Frequencies (Dice Roll)\")\nplt.legend()\nplt.grid(True, axis='y')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}